---
title: "Project 2"
author: "Marcus Hj√∏rund & Gard Gravdal"
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2: default
  bookdown::html_document2: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Problem 1

In this problem we will look at a portion from the Tokyo rainfall dataset, which contains daily rainfall from 1951 to 1989. We consider a response to be whether the amount of rainfall exceeded 1mm over the given time period:

\begin{equation}
y_t|x_t \sim \text{Bin}(n_t, \pi (x_t)), \; \pi (x_t) = \frac{\exp (x_t)}{1 + \exp (x_t)} = \frac{1}{1 + \exp (-x_t)},(\#eq:y-t-x-t) 
\end{equation}

for $n_t$ being $10$ for $t = 60$ (Feburary 29th) and 39 for all other days. $\pi (x_t)$ is the probability of rainfall exceeding 1mm of days $t = 1,\ldots , T$ and $T = 366$. Note that $x_t$ is the logit probability of exceedence and can be obtained from $\pi (x_t)$ via $x_t = \text{log}(\pi (x_t)/(1-\pi (x_t)))$. We assume conditional independence among the $y_t | x_t$ for all $t = 1, \ldots , 366$. 

## a)

```{r, include = TRUE, echo = FALSE}
setwd("C:/Users/marcu/OneDrive/Dokumenter/8. semester/Beregningskrevende statistiske metoder/Project2Beregningskrevende")
load("~/8. semester/Beregningskrevende statistiske metoder/Project2Beregningskrevende/rain.rda")
#rain$day is t
#rain$n.years n_t
#rain$n.rain y_t
library(ggplot2)
```

```{r tokyo-rain,fig.cap = 'Tokyo rain data', fig.dim = c(5,3),include = TRUE, echo = FALSE} 
rain_data <- data.frame(t = rain$day, n_t = rain$n.years, y_t = rain$n.rain)
ggplot(data = rain_data, aes(x = t, y = y_t)) +
  geom_line() + 
  labs(y = expression(paste(y[t]))) + 
  theme_bw()
# pi = rain_data$y_t/rain_data$n_t
# plot(pi, type = "l")
```

As we can see in Figure \@ref(fig:tokyo-rain) the rain in Tokyo seems to show seasonal trend. There are few occurences of rain more than 1mm in the period December-January, the peak of rainfall is in June-July, with a following drier period in August-September. 

## b)

Next we want to obtain an expression for the likelihood of $y_t$ depending on the parameters $\pi (x_t)$ for $t = 1, \ldots , 366$. This gives the likeihood for the data $\boldsymbol{y}$ given the probabilities $\pi(\boldsymbol{x})$, where we will later update $\boldsymbol{x}$ using our a MCMC algorithm. Since we assume conditional intependence for $y_t|x_t$ we can write the likehood, using the expression for $\pi (x_t)$ given in Equation \@ref(eq:y-t-x-t):

\begin{align}
L(\pi (\boldsymbol{x}) |\boldsymbol{y}) &= \prod _{t = 1}^T {n_t \choose y_t} \left(\frac{\exp (x_t)}{1+\exp (x_t)}\right)^{y_t}\left(1-\frac{\exp (x_t)}{1+\exp (x_t)}\right) ^{n_t-y_t} \nonumber \\
 &= \prod _{t = 1}^T {n_t \choose y_t} \left(\frac{\exp (x_t)}{1+\exp (x_t)}\right)^{y_t}\left(\frac{1}{1+\exp (x_t)}\right) ^{n_t-y_t} \nonumber \\
 &= \prod _{t = 1}^T {n_t \choose y_t} \frac{\exp (x_t y_t)}{(1 + \exp (x_t))^{n_t}}
\end{align}

## c)

Now we want to apply a Bayesian hierarchical model to the dataset, where we use a random walk of order 1 (RW(1)) to model the trend on a logit scale,
\begin{equation*}
x_t = x_{t-1} + u_t ,
\end{equation*}
for $u_t \overset{iid}{\sim}\mathcal{N}(0, \sigma _u^2)$ so that,
\begin{equation}
p(\boldsymbol{x}|\sigma _u^2) \propto \prod_{t = 2}^T \frac{1}{\sigma _u}\exp \left\{-\frac{1}{2\sigma _u^2}(x_t - x_{t-1})^2\right\}.
(\#eq:x-likli)
\end{equation}

Then we place a inverse gamma prior on $\sigma _u^2$ such that, 

\begin{equation*}
p(\sigma _u^2) = \frac{\beta ^{\alpha}}{\Gamma (\alpha)}(1/\sigma _u^2)^{\alpha +1} \exp (-\beta /\sigma _u^2)
\end{equation*}
for shape and scale $\alpha$ and $\beta$. We let $\boldsymbol{y} = (y_1, y_2, \ldots ,y_T)^T$, $\boldsymbol{x} = (x_1, x_2 , \ldots , x_T)^T$ and $\boldsymbol{\pi} = (\pi (x_1), \pi (x_2), \ldots , \pi (x_T))^T$.


Now we will derive the posteriori distribution for $\sigma _u^2|\boldsymbol{x}$, which is used in Gibbs sampling in the hybrid sampler in Section 1.6. If we draw a directed acyclical graph for the hierarchical Bayesian model, we see that $\boldsymbol{y}$ is independent of $\sigma _u^2$, as all information from $\boldsymbol{y}$ is stored in $\boldsymbol{x}$. From Bayes theorem we know that 
\begin{equation*}
p(\sigma _u^2|\boldsymbol{y}, \boldsymbol{x}) \propto p(\sigma _u^2) p(\boldsymbol{x}|\sigma _u^2).
\end{equation*}
We then have that 

\begin{align}
p(\sigma _u^2|\boldsymbol{x}, \boldsymbol{y}) &\propto \frac{\beta ^{\alpha}}{\Gamma (\alpha)}(1/\sigma _u^2)^{\alpha +1} \exp (-\beta /\sigma _u^2) \prod _{t = 2}^T\frac{1}{\sigma _u}\exp \left\{-\frac{1}{2\sigma _u^2}(x_t - x_{t-1})^2\right\} \nonumber \\
&= \frac{\beta ^{\alpha}}{\Gamma (\alpha)}(1/\sigma _u^2)^{\alpha +1} \exp (-\beta /\sigma _u^2) \cdot (\sigma _u^2)^{-\frac{T-1}{2}}\exp \left\{-\frac{1}{2\sigma _u^2} \sum_{t=2}^{T}(x_t - x_{t-1})^2\right\} \nonumber \\
&\propto (\sigma _u^2)^{-(\alpha + \frac{T-1}{2}+1)}\exp \left\{-\frac{1}{\sigma_u^2}\left(\beta + \frac{1}{2}\sum_{t = 2}^T(x_t - x_{t-1})^2\right)\right\}
\end{align}
which follows an inverse gamma distribution with parameters 
\begin{equation*}
\alpha ^* = \alpha +\frac{T-1}{2}\; \text{and} \; \beta ^* = \beta + \frac{1}{2}\sum_{t = 2}^T(x_t - x_{t-1})^2.
\end{equation*}

## d)

In the Metropolis-Hastings algorithm, which we will implement, we need to find an expression for the acceptance probability $\alpha$, which in general is given as 
\begin{equation*}
\alpha (y|x) = \min \left\{1, \frac{\pi (y) Q(x|y)}{\pi (x)Q(y|x)}\right\}
\end{equation*}
where $Q(x|y)$ is a proposed conditional probability mass function (pmf), $\pi (x)$ is the probability of being in state $x$, and the transition probability is given as 
\begin{equation*}
p(y|x) = \begin{cases}
Q(y|x)\alpha(y|x) & \text{for } y \neq x  \\
1- \sum_{y \neq x}Q(y|x)\alpha (y|x) &\text{for } y = x \end{cases}
\end{equation*}
which is the probability of transitioning to state $y$ given that the current state is $x$. The expression for $\alpha$ can be derived from the detailed balance equation, $\pi (x) p(y|x) = \pi (y) p(x|y)$ which is true for a time reversible Markov chain. 

We will now consider the conditional prior proposal distribution, $Q(\boldsymbol{x}_\mathcal{I}'|\boldsymbol{x}_{-\mathcal{I}},\sigma _u^2, \boldsymbol{y}) = p(\boldsymbol{x}_\mathcal{I}'|\boldsymbol{x}_{-\mathcal{I}},\sigma _u^2)$, where $\boldsymbol{x}_\mathcal{I}'$ is the proposed values for $\boldsymbol{x}_\mathcal{I}$, $\mathcal{I} \subseteq \{1,\ldots ,366\}$ is a set of time indices, and $\boldsymbol{x}_{-\mathcal{I}}= \boldsymbol{x}_{\{1,\ldots ,366\}\backslash \mathcal{I}}$ is a subset of $\boldsymbol{x}$ that includes all other indices than those in $\mathcal{I}$. Our expression for the acceptance probability is then 
\begin{align*}
\alpha (\boldsymbol{x}_\mathcal{I}'|\boldsymbol{x}_{-\mathcal{I}}, \sigma _u^2, \boldsymbol{y}) &= \min \left\{1, \frac{\pi(\boldsymbol{x}_\mathcal{I}'|\boldsymbol{x}_{-\mathcal{I}},\sigma _u^2, \boldsymbol{y})Q(\boldsymbol{x}_{\mathcal{I}}|\boldsymbol{x}_\mathcal{I}', \boldsymbol{x}_{-\mathcal{I}}, \sigma _u^2, \boldsymbol{y})}{\pi(\boldsymbol{x}_\mathcal{I}|\boldsymbol{x}_{-\mathcal{I}},\sigma _u^2, \boldsymbol{y})Q(\boldsymbol{x}_{\mathcal{I}}'|\boldsymbol{x}_\mathcal{I}, \boldsymbol{x}_{-\mathcal{I}}, \sigma _u^2, \boldsymbol{y})}\right\} \nonumber \\
&= \min \left\{1, \frac{p(\boldsymbol{x}_\mathcal{I}'|\boldsymbol{x}_{-\mathcal{I}},\sigma _u^2, \boldsymbol{y})p(\boldsymbol{x}_{\mathcal{I}}|\boldsymbol{x}_{-\mathcal{I}}, \sigma _u^2)}{p(\boldsymbol{x}_\mathcal{I}|\boldsymbol{x}_{-\mathcal{I}},\sigma _u^2, \boldsymbol{y})p(\boldsymbol{x}_{\mathcal{I}}'|\boldsymbol{x}_{-\mathcal{I}}, \sigma _u^2)}\right\}. 
\end{align*}
We can simplify this expression by looking at $p(\boldsymbol{x}_\mathcal{I}|\boldsymbol{x}_{-\mathcal{I}},\sigma _u^2, \boldsymbol{y})$ and $p(\boldsymbol{x}_\mathcal{I}'|\boldsymbol{x}_{-\mathcal{I}},\sigma _u^2, \boldsymbol{y})$. From probability theory we know that $P(A |B) = \frac{P(A\cap B)}{P(B)}$, such that we can divide the expression of $p(\boldsymbol{x}_\mathcal{I}'|\boldsymbol{x}_{-\mathcal{I}},\sigma _u^2, \boldsymbol{y})$ as follows:
\begin{align*}
p(\boldsymbol{x}_\mathcal{I}'|\boldsymbol{x}_{-\mathcal{I}},\sigma _u^2, \boldsymbol{y}) &= \frac{p(\boldsymbol{x}_\mathcal{I}',\boldsymbol{y}|\boldsymbol{x}_{-\mathcal{I}},\sigma _u^2)}{p(\boldsymbol{y}|\boldsymbol{x}_{-\mathcal{I}},\sigma _u^2)} = \frac{p(\boldsymbol{y}|\boldsymbol{x}_\mathcal{I}',\boldsymbol{x}_{-\mathcal{I}},\sigma _u^2)\cdot p(\boldsymbol{x}_\mathcal{I}'|\boldsymbol{x}_{-\mathcal{I}}, \sigma _u^2)}{p(\boldsymbol{y}|\boldsymbol{x}_{-\mathcal{I}},\sigma _u^2)}\\
&= \frac{p(\boldsymbol{y}_\mathcal{I}|\boldsymbol{x}_\mathcal{I}')\cdot p(\boldsymbol{y}_{-\mathcal{I}}|\boldsymbol{x}_{-\mathcal{I}})\cdot p(\boldsymbol{x}_\mathcal{I}'|\boldsymbol{x}_{-\mathcal{I}},\sigma _u^2)}{p(\boldsymbol{y}|\boldsymbol{x}_{-\mathcal{I}})}
\end{align*}
where we have used that $\boldsymbol{y}$ is independent of $\sigma _u^2$ and that $\boldsymbol{x}_\mathcal{I}$ and $\boldsymbol{x}_{-\mathcal{I}}$ are disjoint sets, and $y_t|x_t$ are conditionally independent. We perform a similar calculation for $p(\boldsymbol{x}_\mathcal{I}|\boldsymbol{x}_{-\mathcal{I}},\sigma _u^2, \boldsymbol{y})$ and obtain 
\begin{equation*}
p(\boldsymbol{x}_\mathcal{I}|\boldsymbol{x}_{-\mathcal{I}},\sigma _u^2, \boldsymbol{y}) = \frac{p(\boldsymbol{y}_\mathcal{I}|\boldsymbol{x}_\mathcal{I})\cdot p(\boldsymbol{y}_{-\mathcal{I}}|\boldsymbol{x}_{-\mathcal{I}})\cdot p(\boldsymbol{x}_\mathcal{I}|\boldsymbol{x}_{-\mathcal{I}},\sigma _u^2)}{p(\boldsymbol{y}|\boldsymbol{x}_{-\mathcal{I}})}.
\end{equation*}
We can then simplify $\alpha (\boldsymbol{x}_\mathcal{I}'|\boldsymbol{x}_{-\mathcal{I}}, \sigma _u^2, \boldsymbol{y})$, which becomes 
\begin{equation}
\alpha (\boldsymbol{x}_\mathcal{I}'|\boldsymbol{x}_{-\mathcal{I}}, \sigma _u^2, \boldsymbol{y}) = \min \left\{1, \frac{p(\boldsymbol{x}_\mathcal{I}'|\boldsymbol{x}_{-\mathcal{I}},\sigma _u^2, \boldsymbol{y})p(\boldsymbol{x}_{\mathcal{I}}|\boldsymbol{x}_{-\mathcal{I}}, \sigma _u^2)}{p(\boldsymbol{x}_\mathcal{I}|\boldsymbol{x}_{-\mathcal{I}},\sigma _u^2, \boldsymbol{y})p(\boldsymbol{x}_{\mathcal{I}}'|\boldsymbol{x}_{-\mathcal{I}}, \sigma _u^2)}\right\} = \min \left\{1, \frac{p(\boldsymbol{y}_\mathcal{I}|\boldsymbol{x}_\mathcal{I}')}{p(\boldsymbol{y}_\mathcal{I}|\boldsymbol{x}_\mathcal{I})}\right\}.
(\#eq:alpha-acceptance-general)
\end{equation}
We can then see that $\alpha (\boldsymbol{x}_\mathcal{I}'|\boldsymbol{x}_{-\mathcal{I}}, \sigma _u^2, \boldsymbol{y})$ becomes a ratio of likelihoods for the proposed values $\boldsymbol{x}_\mathcal{I}'$ against the existing values $\boldsymbol{x}_\mathcal{I}$.

## e)

We note that the density specified by \@ref(eq:x-likli) is improper, for example for $T = 2$, density takes the shape of an infinite 'Gaussian ridge' centered around the line given by $x_1 = x_2$. Equation \@ref(eq:x-likli) can be rewritten as 
\begin{equation}
p(\boldsymbol{x}|\sigma _u^2) \propto \exp \left\{-\frac{1}{2}\boldsymbol{x}^T\mathbf{Q}\boldsymbol{x}\right\}
\end{equation}
which resembles a multivariate normal density but where the improperiety of the density translates into the precision matrix 
\begin{equation*}
\mathbf{Q} = \frac{1}{\sigma _u^2}
\begin{pmatrix}
1 & -1 & & & \\
-1 & 2 & -1 & & \\
& & \ddots  & & \\
& & -1 & 2 & -1 \\
& & & -1 & 1
\end{pmatrix},
\end{equation*}
having one zero eigenvalue. We partition the components of $\boldsymbol{x}$ into two subvectors writing 
\begin{equation*}
\boldsymbol{x} = \begin{pmatrix} \boldsymbol{x}_A \\ \boldsymbol{x}_B \end{pmatrix},
\end{equation*}
and partitioning the precision matrix in the same way as 
\begin{equation*}
\mathbf{Q}= \begin{pmatrix} \mathbf{Q}_{AA} & \mathbf{Q}_{AB} \\
\mathbf{Q}_{BA} & \mathbf{Q}_{BB} \end{pmatrix}.
\end{equation*}
We then want to derive the conditional distribution of $\boldsymbol{x}_A$ conditional on $\boldsymbol{x}_B$. We know from before that the conditional density is always proportional to the joint (improper) density. We assume without loss of generality that $\boldsymbol{\mu} = \boldsymbol{0}$ and find that 
\begin{align*}
f_{\boldsymbol{x}_A|\boldsymbol{x}_B}(\boldsymbol{x}_A) &\propto \exp \left\{-\frac{1}{2}(\boldsymbol{x}_A^T,\boldsymbol{x}_B^T)\begin{bmatrix} \mathbf{Q}_{AA} & \mathbf{Q}_{AB} \\ \mathbf{Q}_{BA} & \mathbf{Q}_{BB} \end{bmatrix}  \begin{pmatrix} \boldsymbol{x}_A \\ \boldsymbol{x}_B \end{pmatrix}\right\} \\
&= \exp \left\{-\frac{1}{2}(\boldsymbol{x}_A^T\mathbf{Q}_{AA}\boldsymbol{x}_A + \boldsymbol{x}_B^T\mathbf{Q}_{BA}\boldsymbol{x}_A + \boldsymbol{x}_A^T\mathbf{Q}_{AB}\boldsymbol{x}_B + \boldsymbol{x}_B^T\mathbf{Q}_{BB}\boldsymbol{x}_B)\right\} \\
&\propto \exp \left\{-\frac{1}{2}(\boldsymbol{x}_A - \boldsymbol{\mu}_{A|B})^T\mathbf{Q}_{AA}(\boldsymbol{x}_B -\boldsymbol{\mu}_{A|B})\right\} \\
&\propto \exp \left\{-\frac{1}{2}(\boldsymbol{x}_A^T\mathbf{Q}_{AA}\boldsymbol{x}_A - \boldsymbol{\mu}_{A|B}^T\mathbf{Q}_{AA}\boldsymbol{x}_A - \boldsymbol{x}_A^T\mathbf{Q}_{AA}\boldsymbol{\mu}_{A|B})\right\}
\end{align*}
where we only consider terms involving $\boldsymbol{x}_A$, which is random in this case, and we use the fact that we can write the terms involving $\boldsymbol{x}_A$ in the second line in quadratic form, completing the square. We then equate the coefficients in the last line to the terms in the second line to find our expressions for $\boldsymbol{\mu}_{A|B}$ and $\mathbf{Q}_{A|B}$. We have 
\begin{align}
-\mathbf{Q}_{AA}\boldsymbol{\mu}_{A|B} = \mathbf{Q}_{AB}\boldsymbol{x}_B \implies \boldsymbol{\mu}_{A|B} &= -\mathbf{Q}_{AA}^{-1}\mathbf{Q}_{AB}\boldsymbol{x}_B (\#eq:conditional-mean)\\ 
\mathbf{Q}_{A|B} &= \mathbf{Q}_{AA} (\#eq:conditional-inverse-variance).
\end{align}
These results is used to simulate from $\boldsymbol{x}_\mathcal{I}'|\boldsymbol{x}_{-\mathcal{I}}$ later on when we generate realizations from the random walk. 

## f)

In this section we want to implement an MCMC sampler from the posterior distribution $p(\boldsymbol{\pi}, \sigma _u^2|\boldsymbol{y})$ using Metropolis-Hastings steps for individual $x_t$ parameters using the conditional prior, $p(x_t|\boldsymbol{x}_{-t},\sigma _u^2)$, and Gibbs steps for $\sigma _u^2$. 

We will begin by deriving the conditional prior distribution using the results from Equations \@ref(eq:conditional-mean) and \@ref(eq:conditional-inverse-variance) as the conditional mean and precision (inverse variance) of $x_\mathcal{I}'|\boldsymbol{x}_{-\mathcal{I}},\sigma _u^2$. We need to derive the conditional prior distribution for three cases, $t = 1$, $2 \leq t \leq 365$ and $t = 366$. For the case $t = 1$, $\mathcal{I} = \{1\}$, we have for the conditional mean that $-\mathbf{Q}_{\mathcal{I}\mathcal{I}}^{-1} = -\sigma _u^2$, $\mathbf{Q}_{\mathcal{I}-\mathcal{I}} = \frac{1}{\sigma _u^2}(-1,0,\dots,0)$ with dimension $1\times (T-1)$. Thus the conditional expectation is given as 
\begin{equation}
x_\mathcal{I}|\boldsymbol{x}_{-\mathcal{I}},\sigma _u^2 = -\mathbf{Q}_{\mathcal{I}\mathcal{I}}^{-1}\mathbf{Q}_{\mathcal{I}-\mathcal{I}}\boldsymbol{x}_{-\mathcal{I}} = -\sigma _u^2 \cdot \frac{1}{\sigma _u^2}(-1,0,\dots,0)(x_2, x_3, \ldots , x_T)^T = x_2.
\end{equation}
Similarly we have for $t = 366$, $\mathcal{I} = \{366\}$, we have 
\begin{equation}
x_\mathcal{I}|\boldsymbol{x}_{-\mathcal{I}},\sigma _u^2 =  -\sigma _u^2 \cdot \frac{1}{\sigma _u^2}(0,\dots,0,-1)(x_1,\ldots ,x_{364}, x_{365})^T = x_{365}.
\end{equation}
Then finally for the interior points, $2 \leq t \leq 365$, $\mathcal{I} =  \{2\}, \{3\},\ldots, \{365\}$, we have $-\mathbf{Q}_{\mathcal{I}\mathcal{I}}^{-1} = -\frac{\sigma _u^2}{2}$ and $\mathbf{Q}_{\mathcal{I}-\mathcal{I}} = \frac{1}{\sigma _u^2}(0,\dots,0,-1,-1,0,\dots,0)$ with dimension $1\times (T-1)$. We then calculate the conditional expectation as 
\begin{equation}
x_\mathcal{I}|\boldsymbol{x}_{-\mathcal{I}},\sigma _u^2 = -\frac{\sigma _u^2}{2} \cdot \frac{1}{\sigma _u^2}(0,\dots,0,-1,-1,0,\dots,0)(x_1,\ldots ,x_{t-1}, x_{t+1}, \ldots, x_{366})^T = \frac{1}{2}(x_{t-1}+x_{t+1}).
\end{equation}
We perform a similar calculation to calculate the conditional variance. We have that the conditional variance $\mathbf{\Sigma}_{\mathcal{I}|-\mathcal{I},\sigma _u^2} = \mathbf{Q}_{\mathcal{I}|-\mathcal{I},\sigma _u^2}^{-1}$. For $t = 1$ and $t = 366$ we have that $\mathbf{Q}_{\mathcal{I}\mathcal{I}}^{-1} = \sigma _u^2$ and for $2\leq t \leq 365$ we have $\mathbf{Q}_{\mathcal{I}\mathcal{I}}^{-1} = \frac{\sigma _u^2}{2}$. Thus we can summarize the conditional distribution of $x_\mathcal{I}|\boldsymbol{x}_{-\mathcal{I}}$ as 
\begin{equation}
x_\mathcal{I}|\boldsymbol{x}_{-\mathcal{I}},\sigma _u^2 \sim \begin{cases}
\mathcal{N}(x_2,\sigma _u^2) &\text{for} \; t = 1\\
\mathcal{N}(\frac{1}{2}(x_{t-1}+x_{t+1}), \frac{\sigma _u^2}{2}) &\text{for} \; t = 2, \ldots ,365 \\
\mathcal{N}(x_{365}, \sigma _u^2) &\text{for} \; t = 366. \end{cases}
\end{equation}

This is the distribution we simulate from on the random walk to generate realizations of $x_t'$, which is the proposed value for $x$ at index $t$. Next we will derive the expression for the acceptance probability $\alpha (x_t'|\boldsymbol{x}_{-t},\sigma _u^2, \boldsymbol{y})$ for $\mathcal{I}  = \{t\}$. We use the expression for the acceptance probability derived in Equation \@ref(eq:alpha-acceptance-general). We have 
\begin{align*}
\alpha (x_t'|\boldsymbol{x}_{-t},\sigma _u^2, \boldsymbol{y}) &= \min \left\{1, \frac{p(y_t|x_t')}{p(y_t|x_t)}\right\} = \min \left\{1,\frac{{n_t \choose y_t} \frac{\exp (x_t' y_t)}{(1 + \exp (x_t'))^{n_t}}}{{n_t \choose y_t} \frac{\exp (x_t y_t)}{(1 + \exp (x_t))^{n_t}}}\right\} \\
&= \min \left\{1, \exp \left(y_t(x_t'-x_t)\cdot \frac{(1+\exp (x_t))^{n_t}}{(1+\exp (x_t'))^{n_t}}\right)\right\}\\ 
&= \min \left\{1, \exp \left(y_t(x_t'-x_t)- n_t \text{ln}\left(\frac{1+\exp (x_t')}{1+\exp (x_t)}\right)\right)\right\}
\end{align*}

In the algorithm, we will for each $t \in \{1,\ldots, 366\}$, use the Metropolis-Hastings algorithm, using the following algorithm, 

**Hybrid sampler Metropolis-Hastings**:

Initialize $x = x_0$ and $\sigma _u^2 \sim p(\sigma _u^2)$

repeat $n$ times:

$\qquad$ for $t = 1, \ldots ,366$:

$\qquad \qquad$ generate $x_t'\sim p(\sigma _u^2|\boldsymbol{x}_{-t})$

$\qquad \qquad$ $\alpha \leftarrow \min \left\{1, \exp \left(y_t(x_t'-x_t)- n_t \text{ln}\left(\frac{1+\exp (x_t')}{1+\exp (x_t)}\right)\right)\right\}$

$\qquad \qquad$ generate $u \sim \text{unif}(0,1)$

$\qquad \qquad$ if $(u < \alpha)$:

$\qquad \qquad \qquad$ $x_t \leftarrow x_t'$

$\qquad \qquad$ else

$\qquad \qquad \qquad$ $x_t \leftarrow x_t$ 

$\qquad$ end for

$\qquad$ $\beta ^* = \beta  + \frac{1}{2}\sum_{t = 2}^T(x_t-x_{t-1})^2$

$\qquad$ generate $\sigma _u^2 \sim \text{InvGamma}(\alpha + \frac{T-1}{2}, \beta ^*)$

end for

Return samples $x_{1},...,x_{T}$

```{r}
# Problem 1
library(matrixStats)
library(MASS)

expit <- function(x){
  return(exp(x)/(1 + exp(x)))
}

# f)

# Defining global variables
alpha = 2
beta = 0.05
N = 50000

alpha_prob <- function(x_prop, x, t){
  # Calculates acceptance probability
  y = rain$n.rain[t]
  n = rain$n.years[t]
  ans = exp(y*(x_prop - x) - n * log((1+exp(x_prop))/(1+exp(x))))
  return(min(1,ans))
}

MH_step <- function(x, sigma2, accept_count, mu, t){
  # Calculates one step of Metropolis-Hastings
  # All inputs are numbers
  x_prop <- rnorm(1, mu, sqrt(sigma2))

  # Probability of acceptance
  alpha_accept <- alpha_prob(x_prop, x, t)
  u <- runif(1)
  
  if (alpha_accept > u){
    x <- x_prop
    accept_count = accept_count + 1
  }
  return(list(x = x, accept_count = accept_count))
  
}

moments_prop <- function(x,t,sigma2, T_max = 366){
  # Returns mu proposed as defined in the text
  # Input x is (1xT_max), t is a number
  if (t == 1){
    return(list(mu= x[2],sigma2 = sigma2 ))
  } 
  else if (t == T_max) {
    return(list(mu = x[T_max-1], sigma2 = sigma2))
  } 
  else{
    return(list(mu = 0.5*(x[t-1] + x[t+1]), sigma2 = sigma2/2))
  }
}

MCMC <- function(n = N, T_max = 366){
  # Time start
  time = proc.time()[3]
  # Initialization
  accept_count = 0
  x <- rep(1,T_max*n)
  x <- matrix(x, nrow = n, ncol = T_max)
  sigma2 <- rep(1,n)
  sigma2[1] <- rgamma(1, alpha, scale = beta)^-1
  for (i in 2:n){
    # One iteration of MCMC:
    old_sigma2 = sigma2[i-1]
    for (t in 1:T_max){
      param = moments_prop(x[i-1,],t,old_sigma2, T_max = T_max)
      mu_temp = param$mu
      sigma2_temp = param$sigma2
      MH_step_t <- MH_step(x[i-1,t], sigma2_temp, accept_count, mu_temp, t)
      x[i,t] <- MH_step_t$x
      accept_count <- MH_step_t$accept_count
    }
    
    # Gibbs step, updating sigma2
    z = beta + 0.5*sum((x[i,-T_max] - x[i, -1])^2)
    sigma2[i] <- 1/rgamma(1,alpha + (T_max-1)/2, scale = z)
  }
  accept_rate = accept_count/ (n * T_max)
  total_time = proc.time()[3] - time
  
  return(list(x = x, total_time = total_time, accept_rate = accept_rate, sigma2 = sigma2))
}

MCMC_run <- MCMC()

MCMC_run$total_time
MCMC_run$accept_rate
```

```{r}
burnin <- ceiling(N/10)
interval <- burnin:N
x_vec = seq(1,N)

plot(x_vec, expit(MCMC_run$x[,1]), ylim = c(0,1), ylab = "y", xlab = "x", type = "l")
plot(x_vec, expit(MCMC_run$x[,201]), ylim = c(0,1), ylab = "y", xlab = "x", type = "l")
plot(x_vec, expit(MCMC_run$x[,366]), ylim = c(0,1), ylab = "y", xlab = "x", type = "l")
plot(x_vec, MCMC_run$sigma2, ylim = c(0,0.01), ylab = "y", xlab = "x", type = "l")

# Plotting histograms
# General histogram function
plot_hist <- function(x, xlab = "x", ylab = "Density", breaks = 50, xlim = c(0,1), alpha = 0.05){
  # Plots general histogram with quantiles and mean
  quantiles <- quantile(x, probs = c(alpha/2, 1 - alpha/2))
  hist(x, breaks = breaks, xlab = xlab, ylab = ylab, xlim = xlim, main = NULL)
  abline(v = quantiles[1], col = "blue")
  abline(v = quantiles[2], col = "blue")
  abline(v = mean(x), col = "red")
  legend("right",legend = c("CI", "Mean", "Samples"), col = c("blue", "red", "grey"), lty = 1)
}
```


```{r}
# Plots
xlab1 = expression(paste(pi, "(x"[1],")"))
xlab2 = expression(paste(pi, "(x"[201],")"))
xlab3 = expression(paste(pi, "(x"[366],")"))
xlab4 = expression(paste(sigma[u]^2))
par(mfrow = c(2, 2), mar = c(5,4,4,2))
hist(expit(MCMC_run$x[,1])[interval], breaks = 50, xlab = xlab1,ylab = "Density", xlim = c(0,0.5), main = NULL)
hist(expit(MCMC_run$x[,1])[interval], breaks = 50, xlab = xlab2,ylab = "Density", xlim = c(0,0.5), main = NULL)
hist(expit(MCMC_run$x[,1])[interval], breaks = 50, xlab = xlab3,ylab = "Density", xlim = c(0,0.5), main = NULL)
plot_hist(MCMC_run$sigma2[interval],xlim = c(0,0.006), xlab = xlab4)
par(mfrow = c(1, 1))


# AVFs:
par(mfrow = c(2, 2), mar = c(5,4,4,2))
acf(expit(MCMC_run$x[,1])[interval], main = xlab1)
acf(expit(MCMC_run$x[,201])[interval], main = xlab2)
acf(expit(MCMC_run$x[,366])[interval], main = xlab3)
acf(MCMC_run$sigma2[interval], main = xlab4)
par(mfrow = c(1, 1), mar = c(5,5,1,1))
```

```{r}
mean_x = colMeans(MCMC_run$x[interval,])
pi_data = rain$n.rain/rain$n.years


x_quantiles <- colQuantiles(MCMC_run$x[interval,], probs = c(0.025, 0.975))
par(mfrow = c(1, 1))
plot(1:366,pi_data, type = "l",xlab = "Day", ylab = expression(paste(pi, "(x)")))
lines(expit(mean_x), type = "l", col = "red")
lines(expit(x_quantiles[,1]), type = "l", col = "blue")
lines(expit(x_quantiles[,2]), type = "l", col = "blue")
legend("topright",legend = c("CI", "Empirical mean", "Data"), col = c("blue", "red", "grey"), lty = 1)
```



# Problem 2

```{r}
head(rain_data)
#install.packages("INLA", repos = c(getOption("repos"), INLA ="https://inla.r-inla-download.org/R/stable"), dep=TRUE)
library("INLA")
t <- proc.time()[3]
control.inla = list(strategy="simplified.laplace", int.strategy="ccd")
mod <- inla(n.rain ~ -1 + f(day, model="rw1", constr=FALSE),
data=rain, Ntrials=n.years, control.compute=list(config = TRUE),
family="binomial", verbose=TRUE, control.inla=control.inla)
run_time <- proc.time()[3]-t
run_time
INLA_fit <- mod$summary.fitted.values$mean
lower <- mod$summary.fitted.values$`0.025quant`
upper <- mod$summary.fitted.values$`0.975quant`
plot(seq(1,366, by = 1), INLA_fit,type = "l", col = "yellow")
lines(expit(mean_x), type = "l", col = "red")

plot(seq(1,366, by = 1), lower, type = "l", col  = "orange")
lines(seq(1,366, by = 1), upper, type = "l", col  ="orange")
lines(expit(x_quantiles[,1]), type = "l", col = "blue")
lines(expit(x_quantiles[,2]), type = "l", col = "blue")

```
```{r}
?control.fixed 
?f
inla.doc("rw1")
inla.doc("X")
```

